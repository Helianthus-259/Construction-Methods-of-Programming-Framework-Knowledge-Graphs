[
  {
    "head": "org.springframework.core.io.buffer.WritableByteChannelSubscriber#currentContext()",
    "headType": "method",
    "relation": "use",
    "tail": "@Override",
    "tailType": "annotation"
  },
  {
    "head": "org.springframework.core.io.buffer.WritableByteChannelSubscriber",
    "headType": "class",
    "relation": "extend",
    "tail": "BaseSubscriber",
    "tailType": "class"
  },
  {
    "head": "org.springframework.core.io.buffer",
    "headType": "package",
    "relation": "haveClass",
    "tail": "org.springframework.core.io.buffer.WriteCompletionHandler",
    "tailType": "class"
  },
  {
    "head": "org.springframework.core.io.buffer.WriteCompletionHandler",
    "headType": "class",
    "relation": "provide",
    "tail": "/*\n * Copyright 2002-2024 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.core.io.buffer;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.AsynchronousFileChannel;\nimport java.nio.channels.Channel;\nimport java.nio.channels.Channels;\nimport java.nio.channels.CompletionHandler;\nimport java.nio.channels.ReadableByteChannel;\nimport java.nio.channels.WritableByteChannel;\nimport java.nio.file.OpenOption;\nimport java.nio.file.Path;\nimport java.nio.file.StandardOpenOption;\nimport java.util.Set;\nimport java.util.concurrent.Callable;\nimport java.util.concurrent.Executor;\nimport java.util.concurrent.Flow;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport java.util.concurrent.atomic.AtomicLong;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.function.Consumer;\n\nimport org.apache.commons.logging.Log;\nimport org.apache.commons.logging.LogFactory;\nimport org.jspecify.annotations.Nullable;\nimport org.reactivestreams.Publisher;\nimport org.reactivestreams.Subscriber;\nimport org.reactivestreams.Subscription;\nimport reactor.core.publisher.BaseSubscriber;\nimport reactor.core.publisher.Flux;\nimport reactor.core.publisher.FluxSink;\nimport reactor.core.publisher.Mono;\nimport reactor.core.publisher.SynchronousSink;\nimport reactor.util.context.Context;\n\nimport org.springframework.core.io.Resource;\nimport org.springframework.util.Assert;\nimport org.springframework.util.CollectionUtils;\n\n/**\n * Utility class for working with {@link DataBuffer DataBuffers}.\n *\n * @author Arjen Poutsma\n * @author Brian Clozel\n * @since 5.0\n */\npublic abstract class DataBufferUtils {\n\n\tprivate static final Log logger = LogFactory.getLog(DataBufferUtils.class);\n\n\tprivate static final Consumer<DataBuffer> RELEASE_CONSUMER = DataBufferUtils::release;\n\n\n\t//---------------------------------------------------------------------\n\t// Reading\n\t//---------------------------------------------------------------------\n\n\t/**\n\t * Obtain an {@link InputStream} from the given supplier, and read it into a\n\t * {@code Flux} of {@code DataBuffer}s. Closes the input stream when the\n\t * Flux is terminated.\n\t * @param inputStreamSupplier the supplier for the input stream to read from\n\t * @param bufferFactory the factory to create data buffers with\n\t * @param bufferSize the maximum size of the data buffers\n\t * @return a Flux of data buffers read from the given channel\n\t */\n\tpublic static Flux<DataBuffer> readInputStream(\n\t\t\tCallable<InputStream> inputStreamSupplier, DataBufferFactory bufferFactory, int bufferSize) {\n\n\t\tAssert.notNull(inputStreamSupplier, \"'inputStreamSupplier' must not be null\");\n\t\treturn readByteChannel(() -> Channels.newChannel(inputStreamSupplier.call()), bufferFactory, bufferSize);\n\t}\n\n\t/**\n\t * Obtain a {@link ReadableByteChannel} from the given supplier, and read\n\t * it into a {@code Flux} of {@code DataBuffer}s. Closes the channel when\n\t * the Flux is terminated.\n\t * @param channelSupplier the supplier for the channel to read from\n\t * @param bufferFactory the factory to create data buffers with\n\t * @param bufferSize the maximum size of the data buffers\n\t * @return a Flux of data buffers read from the given channel\n\t */\n\tpublic static Flux<DataBuffer> readByteChannel(\n\t\t\tCallable<ReadableByteChannel> channelSupplier, DataBufferFactory bufferFactory, int bufferSize) {\n\n\t\tAssert.notNull(channelSupplier, \"'channelSupplier' must not be null\");\n\t\tAssert.notNull(bufferFactory, \"'bufferFactory' must not be null\");\n\t\tAssert.isTrue(bufferSize > 0, \"'bufferSize' must be > 0\");\n\n\t\treturn Flux.using(channelSupplier,\n\t\t\t\tchannel -> Flux.generate(new ReadableByteChannelGenerator(channel, bufferFactory, bufferSize)),\n\t\t\t\tDataBufferUtils::closeChannel);\n\n\t\t// No doOnDiscard as operators used do not cache\n\t}\n\n\t/**\n\t * Obtain a {@code AsynchronousFileChannel} from the given supplier, and read\n\t * it into a {@code Flux} of {@code DataBuffer}s. Closes the channel when\n\t * the Flux is terminated.\n\t * @param channelSupplier the supplier for the channel to read from\n\t * @param bufferFactory the factory to create data buffers with\n\t * @param bufferSize the maximum size of the data buffers\n\t * @return a Flux of data buffers read from the given channel\n\t */\n\tpublic static Flux<DataBuffer> readAsynchronousFileChannel(\n\t\t\tCallable<AsynchronousFileChannel> channelSupplier, DataBufferFactory bufferFactory, int bufferSize) {\n\n\t\treturn readAsynchronousFileChannel(channelSupplier, 0, bufferFactory, bufferSize);\n\t}\n\n\t/**\n\t * Obtain an {@code AsynchronousFileChannel} from the given supplier, and\n\t * read it into a {@code Flux} of {@code DataBuffer}s, starting at the given\n\t * position. Closes the channel when the Flux is terminated.\n\t * @param channelSupplier the supplier for the channel to read from\n\t * @param position the position to start reading from\n\t * @param bufferFactory the factory to create data buffers with\n\t * @param bufferSize the maximum size of the data buffers\n\t * @return a Flux of data buffers read from the given channel\n\t */\n\tpublic static Flux<DataBuffer> readAsynchronousFileChannel(\n\t\t\tCallable<AsynchronousFileChannel> channelSupplier, long position,\n\t\t\tDataBufferFactory bufferFactory, int bufferSize) {\n\n\t\tAssert.notNull(channelSupplier, \"'channelSupplier' must not be null\");\n\t\tAssert.notNull(bufferFactory, \"'bufferFactory' must not be null\");\n\t\tAssert.isTrue(position >= 0, \"'position' must be >= 0\");\n\t\tAssert.isTrue(bufferSize > 0, \"'bufferSize' must be > 0\");\n\n\t\tFlux<DataBuffer> flux = Flux.using(channelSupplier,\n\t\t\t\tchannel -> Flux.create(sink -> {\n\t\t\t\t\tReadCompletionHandler handler =\n\t\t\t\t\t\t\tnew ReadCompletionHandler(channel, sink, position, bufferFactory, bufferSize);\n\t\t\t\t\tsink.onCancel(handler::cancel);\n\t\t\t\t\tsink.onRequest(handler::request);\n\t\t\t\t}),\n\t\t\t\tchannel -> {\n\t\t\t\t\t// Do not close channel from here, rather wait for the current read callback\n\t\t\t\t\t// and then complete after releasing the DataBuffer.\n\t\t\t\t});\n\n\t\treturn flux.doOnDiscard(DataBuffer.class, DataBufferUtils::release);\n\t}\n\n\t/**\n\t * Read bytes from the given file {@code Path} into a {@code Flux} of {@code DataBuffer}s.\n\t * The method ensures that the file is closed when the flux is terminated.\n\t * @param path the path to read bytes from\n\t * @param bufferFactory the factory to create data buffers with\n\t * @param bufferSize the maximum size of the data buffers\n\t * @return a Flux of data buffers read from the given channel\n\t * @since 5.2\n\t */\n\tpublic static Flux<DataBuffer> read(\n\t\t\tPath path, DataBufferFactory bufferFactory, int bufferSize, OpenOption... options) {\n\n\t\tAssert.notNull(path, \"Path must not be null\");\n\t\tAssert.notNull(bufferFactory, \"DataBufferFactory must not be null\");\n\t\tAssert.isTrue(bufferSize > 0, \"'bufferSize' must be > 0\");\n\t\tif (options.length > 0) {\n\t\t\tfor (OpenOption option : options) {\n\t\t\t\tAssert.isTrue(!(option == StandardOpenOption.APPEND || option == StandardOpenOption.WRITE),\n\t\t\t\t\t\t() -> \"'\" + option + \"' not allowed\");\n\t\t\t}\n\t\t}\n\n\t\treturn readAsynchronousFileChannel(() -> AsynchronousFileChannel.open(path, options),\n\t\t\t\tbufferFactory, bufferSize);\n\t}\n\n\t/**\n\t * Read the given {@code Resource} into a {@code Flux} of {@code DataBuffer}s.\n\t * <p>If the resource is a file, it is read into an\n\t * {@code AsynchronousFileChannel} and turned to {@code Flux} via\n\t * {@link #readAsynchronousFileChannel(Callable, DataBufferFactory, int)} or else\n\t * fall back to {@link #readByteChannel(Callable, DataBufferFactory, int)}.\n\t * Closes the channel when the flux is terminated.\n\t * @param resource the resource to read from\n\t * @param bufferFactory the factory to create data buffers with\n\t * @param bufferSize the maximum size of the data buffers\n\t * @return a Flux of data buffers read from the given channel\n\t */\n\tpublic static Flux<DataBuffer> read(Resource resource, DataBufferFactory bufferFactory, int bufferSize) {\n\t\treturn read(resource, 0, bufferFactory, bufferSize);\n\t}\n\n\t/**\n\t * Read the given {@code Resource} into a {@code Flux} of {@code DataBuffer}s\n\t * starting at the given position.\n\t * <p>If the resource is a file, it is read into an\n\t * {@code AsynchronousFileChannel} and turned to {@code Flux} via\n\t * {@link #readAsynchronousFileChannel(Callable, DataBufferFactory, int)} or else\n\t * fall back on {@link #readByteChannel(Callable, DataBufferFactory, int)}.\n\t * Closes the channel when the flux is terminated.\n\t * @param resource the resource to read from\n\t * @param position the position to start reading from\n\t * @param bufferFactory the factory to create data buffers with\n\t * @param bufferSize the maximum size of the data buffers\n\t * @return a Flux of data buffers read from the given channel\n\t */\n\tpublic static Flux<DataBuffer> read(\n\t\t\tResource resource, long position, DataBufferFactory bufferFactory, int bufferSize) {\n\n\t\ttry {\n\t\t\tif (resource.isFile()) {\n\t\t\t\tFile file = resource.getFile();\n\t\t\t\treturn readAsynchronousFileChannel(\n\t\t\t\t\t\t() -> AsynchronousFileChannel.open(file.toPath(), StandardOpenOption.READ),\n\t\t\t\t\t\tposition, bufferFactory, bufferSize);\n\t\t\t}\n\t\t}\n\t\tcatch (IOException ignore) {\n\t\t\t// fallback to resource.readableChannel(), below\n\t\t}\n\t\tFlux<DataBuffer> result = readByteChannel(resource::readableChannel, bufferFactory, bufferSize);\n\t\treturn position == 0 ? result : skipUntilByteCount(result, position);\n\t}\n\n\n\t//---------------------------------------------------------------------\n\t// Writing\n\t//---------------------------------------------------------------------\n\n\t/**\n\t * Write the given stream of {@link DataBuffer DataBuffers} to the given\n\t * {@code OutputStream}. Does <strong>not</strong> close the output stream\n\t * when the flux is terminated, and does <strong>not</strong>\n\t * {@linkplain #release(DataBuffer) release} the data buffers in the source.\n\t * If releasing is required, then subscribe to the returned {@code Flux}\n\t * with a {@link #releaseConsumer()}.\n\t * <p>Note that the writing process does not start until the returned\n\t * {@code Flux} is subscribed to.\n\t * @param source the stream of data buffers to be written\n\t * @param outputStream the output stream to write to\n\t * @return a Flux containing the same buffers as in {@code source}, that\n\t * starts the writing process when subscribed to, and that publishes any\n\t * writing errors and the completion signal\n\t */\n\tpublic static Flux<DataBuffer> write(Publisher<DataBuffer> source, OutputStream outputStream) {\n\t\tAssert.notNull(source, \"'source' must not be null\");\n\t\tAssert.notNull(outputStream, \"'outputStream' must not be null\");\n\n\t\tWritableByteChannel channel = Channels.newChannel(outputStream);\n\t\treturn write(source, channel);\n\t}\n\n\t/**\n\t * Write the given stream of {@link DataBuffer DataBuffers} to the given\n\t * {@code WritableByteChannel}. Does <strong>not</strong> close the channel\n\t * when the flux is terminated, and does <strong>not</strong>\n\t * {@linkplain #release(DataBuffer) release} the data buffers in the source.\n\t * If releasing is required, then subscribe to the returned {@code Flux}\n\t * with a {@link #releaseConsumer()}.\n\t * <p>Note that the writing process does not start until the returned\n\t * {@code Flux} is subscribed to.\n\t * @param source the stream of data buffers to be written\n\t * @param channel the channel to write to\n\t * @return a Flux containing the same buffers as in {@code source}, that\n\t * starts the writing process when subscribed to, and that publishes any\n\t * writing errors and the completion signal\n\t */\n\tpublic static Flux<DataBuffer> write(Publisher<DataBuffer> source, WritableByteChannel channel) {\n\t\tAssert.notNull(source, \"'source' must not be null\");\n\t\tAssert.notNull(channel, \"'channel' must not be null\");\n\n\t\tFlux<DataBuffer> flux = Flux.from(source);\n\t\treturn Flux.create(sink -> {\n\t\t\tWritableByteChannelSubscriber subscriber = new WritableByteChannelSubscriber(sink, channel);\n\t\t\tsink.onDispose(subscriber);\n\t\t\tflux.subscribe(subscriber);\n\t\t});\n\t}\n\n\t/**\n\t * Write the given stream of {@link DataBuffer DataBuffers} to the given\n\t * {@code AsynchronousFileChannel}. Does <strong>not</strong> close the\n\t * channel when the flux is terminated, and does <strong>not</strong>\n\t * {@linkplain #release(DataBuffer) release} the data buffers in the source.\n\t * If releasing is required, then subscribe to the returned {@code Flux}\n\t * with a {@link #releaseConsumer()}.\n\t * <p>Note that the writing process does not start until the returned\n\t * {@code Flux} is subscribed to.\n\t * @param source the stream of data buffers to be written\n\t * @param channel the channel to write to\n\t * @return a Flux containing the same buffers as in {@code source}, that\n\t * starts the writing process when subscribed to, and that publishes any\n\t * writing errors and the completion signal\n\t * @since 5.0.10\n\t */\n\tpublic static Flux<DataBuffer> write(Publisher<DataBuffer> source, AsynchronousFileChannel channel) {\n\t\treturn write(source, channel, 0);\n\t}\n\n\t/**\n\t * Write the given stream of {@link DataBuffer DataBuffers} to the given\n\t * {@code AsynchronousFileChannel}. Does <strong>not</strong> close the channel\n\t * when the flux is terminated, and does <strong>not</strong>\n\t * {@linkplain #release(DataBuffer) release} the data buffers in the source.\n\t * If releasing is required, then subscribe to the returned {@code Flux} with a\n\t * {@link #releaseConsumer()}.\n\t * <p>Note that the writing process does not start until the returned\n\t * {@code Flux} is subscribed to.\n\t * @param source the stream of data buffers to be written\n\t * @param channel the channel to write to\n\t * @param position the file position where writing is to begin; must be non-negative\n\t * @return a flux containing the same buffers as in {@code source}, that\n\t * starts the writing process when subscribed to, and that publishes any\n\t * writing errors and the completion signal\n\t */\n\tpublic static Flux<DataBuffer> write(\n\t\t\tPublisher<? extends DataBuffer> source, AsynchronousFileChannel channel, long position) {\n\n\t\tAssert.notNull(source, \"'source' must not be null\");\n\t\tAssert.notNull(channel, \"'channel' must not be null\");\n\t\tAssert.isTrue(position >= 0, \"'position' must be >= 0\");\n\n\t\tFlux<DataBuffer> flux = Flux.from(source);\n\t\treturn Flux.create(sink -> {\n\t\t\tWriteCompletionHandler handler = new WriteCompletionHandler(sink, channel, position);\n\t\t\tsink.onDispose(handler);\n\t\t\tflux.subscribe(handler);\n\t\t});\n\n\n\t}\n\n\t/**\n\t * Write the given stream of {@link DataBuffer DataBuffers} to the given\n\t * file {@link Path}. The optional {@code options} parameter specifies\n\t * how the file is created or opened (defaults to\n\t * {@link StandardOpenOption#CREATE CREATE},\n\t * {@link StandardOpenOption#TRUNCATE_EXISTING TRUNCATE_EXISTING}, and\n\t * {@link StandardOpenOption#WRITE WRITE}).\n\t * @param source the stream of data buffers to be written\n\t * @param destination the path to the file\n\t * @param options the options specifying how the file is opened\n\t * @return a {@link Mono} that indicates completion or error\n\t * @since 5.2\n\t */\n\tpublic static Mono<Void> write(Publisher<DataBuffer> source, Path destination, OpenOption... options) {\n\t\tAssert.notNull(source, \"Source must not be null\");\n\t\tAssert.notNull(destination, \"Destination must not be null\");\n\n\t\tSet<OpenOption> optionSet = checkWriteOptions(options);\n\n\t\treturn Mono.create(sink -> {\n\t\t\ttry {\n\t\t\t\tAsynchronousFileChannel channel = AsynchronousFileChannel.open(destination, optionSet, null);\n\t\t\t\tsink.onDispose(() -> closeChannel(channel));\n\t\t\t\twrite(source, channel).subscribe(DataBufferUtils::release,\n\t\t\t\t\t\tsink::error,\n\t\t\t\t\t\tsink::success,\n\t\t\t\t\t\tContext.of(sink.contextView()));\n\t\t\t}\n\t\t\tcatch (IOException ex) {\n\t\t\t\tsink.error(ex);\n\t\t\t}\n\t\t});\n\t}\n\n\tprivate static Set<OpenOption> checkWriteOptions(OpenOption[] options) {\n\t\tint length = options.length;\n\t\tSet<OpenOption> result = CollectionUtils.newHashSet(length > 0 ? length : 2);\n\t\tif (length == 0) {\n\t\t\tresult.add(StandardOpenOption.CREATE);\n\t\t\tresult.add(StandardOpenOption.TRUNCATE_EXISTING);\n\t\t}\n\t\telse {\n\t\t\tfor (OpenOption opt : options) {\n\t\t\t\tif (opt == StandardOpenOption.READ) {\n\t\t\t\t\tthrow new IllegalArgumentException(\"READ not allowed\");\n\t\t\t\t}\n\t\t\t\tresult.add(opt);\n\t\t\t}\n\t\t}\n\t\tresult.add(StandardOpenOption.WRITE);\n\t\treturn result;\n\t}\n\n\tstatic void closeChannel(@Nullable Channel channel) {\n\t\tif (channel != null && channel.isOpen()) {\n\t\t\ttry {\n\t\t\t\tchannel.close();\n\t\t\t}\n\t\t\tcatch (IOException ignored) {\n\t\t\t}\n\t\t}\n\t}\n\n\n\t/**\n\t * Create a new {@code Publisher<DataBuffer>} based on bytes written to a\n\t * {@code OutputStream}.\n\t * <ul>\n\t * <li>The parameter {@code outputStreamConsumer} is invoked once per\n\t * subscription of the returned {@code Publisher}, when the first\n\t * item is\n\t * {@linkplain Subscription#request(long) requested}.</li>\n\t * <li>{@link OutputStream#write(byte[], int, int) OutputStream.write()}\n\t * invocations made by {@code outputStreamConsumer} are buffered until they\n\t * exceed the default chunk size of 1024, or when the stream is\n\t * {@linkplain OutputStream#flush() flushed} and then result in a\n\t * {@linkplain Subscriber#onNext(Object) published} item\n\t * if there is {@linkplain Subscription#request(long) demand}.</li>\n\t * <li>If there is <em>no demand</em>, {@code OutputStream.write()} will block\n\t * until there is.</li>\n\t * <li>If the subscription is {@linkplain Subscription#cancel() cancelled},\n\t * {@code OutputStream.write()} will throw a {@code IOException}.</li>\n\t * <li>The subscription is\n\t * {@linkplain Subscriber#onComplete() completed} when\n\t * {@code outputStreamHandler} completes.</li>\n\t * <li>Any exceptions thrown from {@code outputStreamHandler} will\n\t * be dispatched to the {@linkplain Subscriber#onError(Throwable) Subscriber}.\n\t * </ul>\n\t * @param consumer invoked when the first buffer is requested\n\t * @param executor used to invoke the {@code outputStreamHandler}\n\t * @return a {@code Publisher<DataBuffer>} based on bytes written by\n\t * {@code outputStreamHandler}\n\t * @since 6.1\n\t */\n\tpublic static Publisher<DataBuffer> outputStreamPublisher(\n\t\t\tConsumer<OutputStream> consumer, DataBufferFactory bufferFactory, Executor executor) {\n\n\t\treturn new OutputStreamPublisher<>(\n\t\t\t\tconsumer::accept, new DataBufferMapper(bufferFactory), executor, null);\n\t}\n\n\t/**\n\t * Variant of {@link #outputStreamPublisher(Consumer, DataBufferFactory, Executor)}\n\t * providing control over the chunk sizes to be produced by the publisher.\n\t * @since 6.1\n\t */\n\tpublic static Publisher<DataBuffer> outputStreamPublisher(\n\t\t\tConsumer<OutputStream> consumer, DataBufferFactory bufferFactory, Executor executor, int chunkSize) {\n\n\t\treturn new OutputStreamPublisher<>(\n\t\t\t\tconsumer::accept, new DataBufferMapper(bufferFactory), executor, chunkSize);\n\t}\n\n\t/**\n\t * Subscribe to given {@link Publisher} of {@code DataBuffer}s, and return an\n\t * {@link InputStream} to consume the byte content with.\n\t * <p>Byte buffers are stored in a queue. The {@code demand} constructor value\n\t * determines the number of buffers requested initially. When storage falls\n\t * below a {@code (demand - (demand >> 2))} limit, a request is made to refill\n\t * the queue.\n\t * <p>The {@code InputStream} terminates after an onError or onComplete signal,\n\t * and stored buffers are read. If the {@code InputStream} is closed,\n\t * the {@link Flow.Subscription} is cancelled, and stored buffers released.\n\t * @param publisher the source of {@code DataBuffer}s\n\t * @param demand the number of buffers to request initially, and buffer\n\t * internally on an ongoing basis.\n\t * @return an {@link InputStream} backed by the {@link Publisher}\n\t */\n\tpublic static <T extends DataBuffer> InputStream subscriberInputStream(Publisher<T> publisher, int demand) {\n\t\tAssert.notNull(publisher, \"Publisher must not be null\");\n\t\tAssert.isTrue(demand > 0, \"maxBufferCount must be > 0\");\n\n\t\tSubscriberInputStream subscriber = new SubscriberInputStream(demand);\n\t\tpublisher.subscribe(subscriber);\n\t\treturn subscriber;\n\t}\n\n\n\t//---------------------------------------------------------------------\n\t// Various\n\t//---------------------------------------------------------------------\n\n\t/**\n\t * Relay buffers from the given {@link Publisher} until the total\n\t * {@linkplain DataBuffer#readableByteCount() byte count} reaches\n\t * the given maximum byte count, or until the publisher is complete.\n\t * @param publisher the publisher to filter\n\t * @param maxByteCount the maximum byte count\n\t * @return a flux whose maximum byte count is {@code maxByteCount}\n\t */\n\t@SuppressWarnings(\"unchecked\")\n\tpublic static <T extends DataBuffer> Flux<T> takeUntilByteCount(Publisher<T> publisher, long maxByteCount) {\n\t\tAssert.notNull(publisher, \"Publisher must not be null\");\n\t\tAssert.isTrue(maxByteCount >= 0, \"'maxByteCount' must be >= 0\");\n\n\t\treturn Flux.defer(() -> {\n\t\t\tAtomicLong countDown = new AtomicLong(maxByteCount);\n\t\t\treturn Flux.from(publisher)\n\t\t\t\t\t.map(buffer -> {\n\t\t\t\t\t\tlong remainder = countDown.addAndGet(-buffer.readableByteCount());\n\t\t\t\t\t\tif (remainder < 0) {\n\t\t\t\t\t\t\tint index = buffer.readableByteCount() + (int) remainder;\n\t\t\t\t\t\t\tDataBuffer split = buffer.split(index);\n\t\t\t\t\t\t\trelease(buffer);\n\t\t\t\t\t\t\treturn (T)split;\n\t\t\t\t\t\t}\n\t\t\t\t\t\telse {\n\t\t\t\t\t\t\treturn buffer;\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\t\t\t\t\t.takeUntil(buffer -> countDown.get() <= 0);\n\t\t});\n\n\t\t// No doOnDiscard as operators used do not cache (and drop) buffers\n\t}\n\n\t/**\n\t * Skip buffers from the given {@link Publisher} until the total\n\t * {@linkplain DataBuffer#readableByteCount() byte count} reaches\n\t * the given maximum byte count, or until the publisher is complete.\n\t * @param publisher the publisher to filter\n\t * @param maxByteCount the maximum byte count\n\t * @return a flux with the remaining part of the given publisher\n\t */\n\tpublic static <T extends DataBuffer> Flux<T> skipUntilByteCount(Publisher<T> publisher, long maxByteCount) {\n\t\tAssert.notNull(publisher, \"Publisher must not be null\");\n\t\tAssert.isTrue(maxByteCount >= 0, \"'maxByteCount' must be >= 0\");\n\n\t\treturn Flux.defer(() -> {\n\t\t\tAtomicLong countDown = new AtomicLong(maxByteCount);\n\t\t\treturn Flux.from(publisher)\n\t\t\t\t\t.skipUntil(buffer -> {\n\t\t\t\t\t\tlong remainder = countDown.addAndGet(-buffer.readableByteCount());\n\t\t\t\t\t\treturn remainder < 0;\n\t\t\t\t\t})\n\t\t\t\t\t.map(buffer -> {\n\t\t\t\t\t\tlong remainder = countDown.get();\n\t\t\t\t\t\tif (remainder < 0) {\n\t\t\t\t\t\t\tcountDown.set(0);\n\t\t\t\t\t\t\tint start = buffer.readableByteCount() + (int)remainder;\n\t\t\t\t\t\t\tDataBuffer split = buffer.split(start);\n\t\t\t\t\t\t\trelease(split);\n\t\t\t\t\t\t\treturn buffer;\n\t\t\t\t\t\t}\n\t\t\t\t\t\telse {\n\t\t\t\t\t\t\treturn buffer;\n\t\t\t\t\t\t}\n\t\t\t\t\t});\n\t\t}).doOnDiscard(DataBuffer.class, DataBufferUtils::release);\n\t}\n\n\t/**\n\t * Retain the given data buffer, if it is a {@link PooledDataBuffer}.\n\t * @param dataBuffer the data buffer to retain\n\t * @return the retained buffer\n\t */\n\t@SuppressWarnings(\"unchecked\")\n\tpublic static <T extends DataBuffer> T retain(T dataBuffer) {\n\t\tif (dataBuffer instanceof PooledDataBuffer pooledDataBuffer) {\n\t\t\treturn (T) pooledDataBuffer.retain();\n\t\t}\n\t\telse {\n\t\t\treturn dataBuffer;\n\t\t}\n\t}\n\n\t/**\n\t * Associate the given hint with the data buffer if it is a pooled buffer\n\t * and supports leak tracking.\n\t * @param dataBuffer the data buffer to attach the hint to\n\t * @param hint the hint to attach\n\t * @return the input buffer\n\t * @since 5.3.2\n\t */\n\t@SuppressWarnings(\"unchecked\")\n\tpublic static <T extends DataBuffer> T touch(T dataBuffer, Object hint) {\n\t\tif (dataBuffer instanceof TouchableDataBuffer touchableDataBuffer) {\n\t\t\treturn (T) touchableDataBuffer.touch(hint);\n\t\t}\n\t\telse {\n\t\t\treturn dataBuffer;\n\t\t}\n\t}\n\n\t/**\n\t * Release the given data buffer. If it is a {@link PooledDataBuffer} and\n\t * has been {@linkplain PooledDataBuffer#isAllocated() allocated}, this\n\t * method will call {@link PooledDataBuffer#release()}. If it is a\n\t * {@link CloseableDataBuffer}, this method will call\n\t * {@link CloseableDataBuffer#close()}.\n\t * @param dataBuffer the data buffer to release\n\t * @return {@code true} if the buffer was released; {@code false} otherwise.\n\t */\n\tpublic static boolean release(@Nullable DataBuffer dataBuffer) {\n\t\tif (dataBuffer instanceof PooledDataBuffer pooledDataBuffer) {\n\t\t\tif (pooledDataBuffer.isAllocated()) {\n\t\t\t\ttry {\n\t\t\t\t\treturn pooledDataBuffer.release();\n\t\t\t\t}\n\t\t\t\tcatch (IllegalStateException ex) {\n\t\t\t\t\tif (logger.isDebugEnabled()) {\n\t\t\t\t\t\tlogger.debug(\"Failed to release PooledDataBuffer: \" + dataBuffer, ex);\n\t\t\t\t\t}\n\t\t\t\t\treturn false;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\telse if (dataBuffer instanceof CloseableDataBuffer closeableDataBuffer) {\n\t\t\ttry {\n\t\t\t\tcloseableDataBuffer.close();\n\t\t\t\treturn true;\n\t\t\t}\n\t\t\tcatch (IllegalStateException ex) {\n\t\t\t\tif (logger.isDebugEnabled()) {\n\t\t\t\t\tlogger.debug(\"Failed to release CloseableDataBuffer \" + dataBuffer, ex);\n\t\t\t\t}\n\t\t\t\treturn false;\n\n\t\t\t}\n\t\t}\n\t\treturn false;\n\t}\n\n\t/**\n\t * Return a consumer that calls {@link #release(DataBuffer)} on all\n\t * passed data buffers.\n\t */\n\tpublic static Consumer<DataBuffer> releaseConsumer() {\n\t\treturn RELEASE_CONSUMER;\n\t}\n\n\t/**\n\t * Return a new {@code DataBuffer} composed of joining together the given\n\t * {@code dataBuffers} elements. Depending on the {@link DataBuffer} type,\n\t * the returned buffer may be a single buffer containing all data of the\n\t * provided buffers, or it may be a zero-copy, composite with references to\n\t * the given buffers.\n\t * <p>If {@code dataBuffers} produces an error or if there is a cancel\n\t * signal, then all accumulated buffers will be\n\t * {@linkplain #release(DataBuffer) released}.\n\t * <p>Note that the given data buffers do <strong>not</strong> have to be\n\t * released. They will be released as part of the returned composite.\n\t * @param dataBuffers the data buffers that are to be composed\n\t * @return a buffer that is composed of the {@code dataBuffers} argument\n\t * @since 5.0.3\n\t */\n\tpublic static Mono<DataBuffer> join(Publisher<? extends DataBuffer> dataBuffers) {\n\t\treturn join(dataBuffers, -1);\n\t}\n\n\t/**\n\t * Variant of {@link #join(Publisher)} that behaves the same way up until\n\t * the specified max number of bytes to buffer. Once the limit is exceeded,\n\t * {@link DataBufferLimitException} is raised.\n\t * @param buffers the data buffers that are to be composed\n\t * @param maxByteCount the max number of bytes to buffer, or -1 for unlimited\n\t * @return a buffer with the aggregated content, possibly an empty Mono if\n\t * the max number of bytes to buffer is exceeded.\n\t * @throws DataBufferLimitException if maxByteCount is exceeded\n\t * @since 5.1.11\n\t */\n\t@SuppressWarnings({\"rawtypes\", \"unchecked\"})\n\tpublic static Mono<DataBuffer> join(Publisher<? extends DataBuffer> buffers, int maxByteCount) {\n\t\tAssert.notNull(buffers, \"'buffers' must not be null\");\n\n\t\tif (buffers instanceof Mono mono) {\n\t\t\treturn mono;\n\t\t}\n\n\t\treturn Flux.from(buffers)\n\t\t\t\t.collect(() -> new LimitedDataBufferList(maxByteCount), LimitedDataBufferList::add)\n\t\t\t\t.filter(list -> !list.isEmpty())\n\t\t\t\t.map(list -> list.get(0).factory().join(list))\n\t\t\t\t.doOnDiscard(DataBuffer.class, DataBufferUtils::release);\n\t}\n\n\t/**\n\t * Return a {@link Matcher} for the given delimiter.\n\t * The matcher can be used to find the delimiters in a stream of data buffers.\n\t * @param delimiter the delimiter bytes to find\n\t * @return the matcher\n\t * @since 5.2\n\t */\n\tpublic static Matcher matcher(byte[] delimiter) {\n\t\treturn createMatcher(delimiter);\n\t}\n\n\t/**\n\t * Return a {@link Matcher} for the given delimiters.\n\t * The matcher can be used to find the delimiters in a stream of data buffers.\n\t * @param delimiters the delimiters bytes to find\n\t * @return the matcher\n\t * @since 5.2\n\t */\n\tpublic static Matcher matcher(byte[]... delimiters) {\n\t\tAssert.isTrue(delimiters.length > 0, \"Delimiters must not be empty\");\n\t\treturn (delimiters.length == 1 ? createMatcher(delimiters[0]) : new CompositeMatcher(delimiters));\n\t}\n\n\tprivate static NestedMatcher createMatcher(byte[] delimiter) {\n\t\t// extract length due to Eclipse IDE compiler error in switch expression\n\t\tint length = delimiter.length;\n\t\tAssert.isTrue(length > 0, \"Delimiter must not be empty\");\n\t\treturn switch (length) {\n\t\t\tcase 1 -> (delimiter[0] == 10 ? SingleByteMatcher.NEWLINE_MATCHER : new SingleByteMatcher(delimiter));\n\t\t\tcase 2 -> new TwoByteMatcher(delimiter);\n\t\t\tdefault -> new KnuthMorrisPrattMatcher(delimiter);\n\t\t};\n\t}\n\n\n\t/**\n\t * Contract to find delimiter(s) against one or more data buffers that can\n\t * be passed one at a time to the {@link #match(DataBuffer)} method.\n\t *\n\t * @since 5.2\n\t * @see #match(DataBuffer)\n\t */\n\tpublic interface Matcher {\n\n\t\t/**\n\t\t * Find the first matching delimiter and return the index of the last\n\t\t * byte of the delimiter, or {@code -1} if not found.\n\t\t */\n\t\tint match(DataBuffer dataBuffer);\n\n\t\t/**\n\t\t * Return the delimiter from the last invocation of {@link #match(DataBuffer)}.\n\t\t */\n\t\tbyte[] delimiter();\n\n\t\t/**\n\t\t * Reset the state of this matcher.\n\t\t */\n\t\tvoid reset();\n\t}\n\n\n\t/**\n\t * Matcher that supports searching for multiple delimiters.\n\t */\n\tprivate static class CompositeMatcher implements Matcher {\n\n\t\tprivate static final byte[] NO_DELIMITER = new byte[0];\n\n\n\t\tprivate final NestedMatcher[] matchers;\n\n\t\tbyte[] longestDelimiter = NO_DELIMITER;\n\n\t\tCompositeMatcher(byte[][] delimiters) {\n\t\t\tthis.matchers = initMatchers(delimiters);\n\t\t}\n\n\t\tprivate static NestedMatcher[] initMatchers(byte[][] delimiters) {\n\t\t\tNestedMatcher[] matchers = new NestedMatcher[delimiters.length];\n\t\t\tfor (int i = 0; i < delimiters.length; i++) {\n\t\t\t\tmatchers[i] = createMatcher(delimiters[i]);\n\t\t\t}\n\t\t\treturn matchers;\n\t\t}\n\n\t\t@Override\n\t\tpublic int match(DataBuffer dataBuffer) {\n\t\t\tthis.longestDelimiter = NO_DELIMITER;\n\n\t\t\tfor (int pos = dataBuffer.readPosition(); pos < dataBuffer.writePosition(); pos++) {\n\t\t\t\tbyte b = dataBuffer.getByte(pos);\n\n\t\t\t\tfor (NestedMatcher matcher : this.matchers) {\n\t\t\t\t\tif (matcher.match(b) && matcher.delimiter().length > this.longestDelimiter.length) {\n\t\t\t\t\t\tthis.longestDelimiter = matcher.delimiter();\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif (this.longestDelimiter != NO_DELIMITER) {\n\t\t\t\t\treset();\n\t\t\t\t\treturn pos;\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn -1;\n\t\t}\n\n\t\t@Override\n\t\tpublic byte[] delimiter() {\n\t\t\tAssert.state(this.longestDelimiter != NO_DELIMITER, \"'delimiter' not set\");\n\t\t\treturn this.longestDelimiter;\n\t\t}\n\n\t\t@Override\n\t\tpublic void reset() {\n\t\t\tfor (NestedMatcher matcher : this.matchers) {\n\t\t\t\tmatcher.reset();\n\t\t\t}\n\t\t}\n\t}\n\n\n\t/**\n\t * Matcher that can be nested within {@link CompositeMatcher} where multiple\n\t * matchers advance together using the same index, one byte at a time.\n\t */\n\tprivate interface NestedMatcher extends Matcher {\n\n\t\t/**\n\t\t * Perform a match against the next byte of the stream and return true\n\t\t * if the delimiter is fully matched.\n\t\t */\n\t\tboolean match(byte b);\n\n\t}\n\n\n\t/**\n\t * Matcher for a single byte delimiter.\n\t */\n\tprivate static class SingleByteMatcher implements NestedMatcher {\n\n\t\tstatic final SingleByteMatcher NEWLINE_MATCHER = new SingleByteMatcher(new byte[] {10});\n\n\t\tprivate final byte[] delimiter;\n\n\t\tSingleByteMatcher(byte[] delimiter) {\n\t\t\tAssert.isTrue(delimiter.length == 1, \"Expected a 1 byte delimiter\");\n\t\t\tthis.delimiter = delimiter;\n\t\t}\n\n\t\t@Override\n\t\tpublic int match(DataBuffer dataBuffer) {\n\t\t\tfor (int pos = dataBuffer.readPosition(); pos < dataBuffer.writePosition(); pos++) {\n\t\t\t\tbyte b = dataBuffer.getByte(pos);\n\t\t\t\tif (match(b)) {\n\t\t\t\t\treturn pos;\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn -1;\n\t\t}\n\n\t\t@Override\n\t\tpublic boolean match(byte b) {\n\t\t\treturn this.delimiter[0] == b;\n\t\t}\n\n\t\t@Override\n\t\tpublic byte[] delimiter() {\n\t\t\treturn this.delimiter;\n\t\t}\n\n\t\t@Override\n\t\tpublic void reset() {\n\t\t}\n\t}\n\n\n\t/**\n\t * Base class for a {@link NestedMatcher}.\n\t */\n\tprivate abstract static class AbstractNestedMatcher implements NestedMatcher {\n\n\t\tprivate final byte[] delimiter;\n\n\t\tprivate int matches = 0;\n\n\n\t\tprotected AbstractNestedMatcher(byte[] delimiter) {\n\t\t\tthis.delimiter = delimiter;\n\t\t}\n\n\t\tprotected void setMatches(int index) {\n\t\t\tthis.matches = index;\n\t\t}\n\n\t\tprotected int getMatches() {\n\t\t\treturn this.matches;\n\t\t}\n\n\t\t@Override\n\t\tpublic int match(DataBuffer dataBuffer) {\n\t\t\tfor (int pos = dataBuffer.readPosition(); pos < dataBuffer.writePosition(); pos++) {\n\t\t\t\tbyte b = dataBuffer.getByte(pos);\n\t\t\t\tif (match(b)) {\n\t\t\t\t\treset();\n\t\t\t\t\treturn pos;\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn -1;\n\t\t}\n\n\t\t@Override\n\t\tpublic boolean match(byte b) {\n\t\t\tif (b == this.delimiter[this.matches]) {\n\t\t\t\tthis.matches++;\n\t\t\t\treturn (this.matches == delimiter().length);\n\t\t\t}\n\t\t\treturn false;\n\t\t}\n\n\t\t@Override\n\t\tpublic byte[] delimiter() {\n\t\t\treturn this.delimiter;\n\t\t}\n\n\t\t@Override\n\t\tpublic void reset() {\n\t\t\tthis.matches = 0;\n\t\t}\n\t}\n\n\n\t/**\n\t * Matcher with a 2 byte delimiter that does not benefit from a\n\t * Knuth-Morris-Pratt suffix-prefix table.\n\t */\n\tprivate static class TwoByteMatcher extends AbstractNestedMatcher {\n\n\t\tprotected TwoByteMatcher(byte[] delimiter) {\n\t\t\tsuper(delimiter);\n\t\t\tAssert.isTrue(delimiter.length == 2, \"Expected a 2-byte delimiter\");\n\t\t}\n\t}\n\n\n\t/**\n\t * Implementation of {@link Matcher} that uses the Knuth-Morris-Pratt algorithm.\n\t * @see <a href=\"https://www.nayuki.io/page/knuth-morris-pratt-string-matching\">Knuth-Morris-Pratt string matching</a>\n\t */\n\tprivate static class KnuthMorrisPrattMatcher extends AbstractNestedMatcher {\n\n\t\tprivate final int[] table;\n\n\t\tpublic KnuthMorrisPrattMatcher(byte[] delimiter) {\n\t\t\tsuper(delimiter);\n\t\t\tthis.table = longestSuffixPrefixTable(delimiter);\n\t\t}\n\n\t\tprivate static int[] longestSuffixPrefixTable(byte[] delimiter) {\n\t\t\tint[] result = new int[delimiter.length];\n\t\t\tresult[0] = 0;\n\t\t\tfor (int i = 1; i < delimiter.length; i++) {\n\t\t\t\tint j = result[i - 1];\n\t\t\t\twhile (j > 0 && delimiter[i] != delimiter[j]) {\n\t\t\t\t\tj = result[j - 1];\n\t\t\t\t}\n\t\t\t\tif (delimiter[i] == delimiter[j]) {\n\t\t\t\t\tj++;\n\t\t\t\t}\n\t\t\t\tresult[i] = j;\n\t\t\t}\n\t\t\treturn result;\n\t\t}\n\n\t\t@Override\n\t\tpublic boolean match(byte b) {\n\t\t\twhile (getMatches() > 0 && b != delimiter()[getMatches()]) {\n\t\t\t\tsetMatches(this.table[getMatches() - 1]);\n\t\t\t}\n\t\t\treturn super.match(b);\n\t\t}\n\t}\n\n\n\tprivate static class ReadableByteChannelGenerator implements Consumer<SynchronousSink<DataBuffer>> {\n\n\t\tprivate final ReadableByteChannel channel;\n\n\t\tprivate final DataBufferFactory dataBufferFactory;\n\n\t\tprivate final int bufferSize;\n\n\t\tpublic ReadableByteChannelGenerator(\n\t\t\t\tReadableByteChannel channel, DataBufferFactory dataBufferFactory, int bufferSize) {\n\n\t\t\tthis.channel = channel;\n\t\t\tthis.dataBufferFactory = dataBufferFactory;\n\t\t\tthis.bufferSize = bufferSize;\n\t\t}\n\n\t\t@Override\n\t\tpublic void accept(SynchronousSink<DataBuffer> sink) {\n\t\t\tint read = -1;\n\t\t\tDataBuffer dataBuffer = this.dataBufferFactory.allocateBuffer(this.bufferSize);\n\t\t\ttry {\n\t\t\t\ttry (DataBuffer.ByteBufferIterator iterator = dataBuffer.writableByteBuffers()) {\n\t\t\t\t\tAssert.state(iterator.hasNext(), \"No ByteBuffer available\");\n\t\t\t\t\tByteBuffer byteBuffer = iterator.next();\n\t\t\t\t\tread = this.channel.read(byteBuffer);\n\t\t\t\t}\n\t\t\t\tif (read >= 0) {\n\t\t\t\t\tdataBuffer.writePosition(read);\n\t\t\t\t\tsink.next(dataBuffer);\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tsink.complete();\n\t\t\t\t}\n\t\t\t}\n\t\t\tcatch (IOException ex) {\n\t\t\t\tsink.error(ex);\n\t\t\t}\n\t\t\tfinally {\n\t\t\t\tif (read == -1) {\n\t\t\t\t\trelease(dataBuffer);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\n\tprivate static class ReadCompletionHandler implements CompletionHandler<Integer, ReadCompletionHandler.Attachment> {\n\n\t\tprivate final AsynchronousFileChannel channel;\n\n\t\tprivate final FluxSink<DataBuffer> sink;\n\n\t\tprivate final DataBufferFactory dataBufferFactory;\n\n\t\tprivate final int bufferSize;\n\n\t\tprivate final AtomicLong position;\n\n\t\tprivate final AtomicReference<State> state = new AtomicReference<>(State.IDLE);\n\n\t\tpublic ReadCompletionHandler(AsynchronousFileChannel channel,\n\t\t\t\tFluxSink<DataBuffer> sink, long position, DataBufferFactory dataBufferFactory, int bufferSize) {\n\n\t\t\tthis.channel = channel;\n\t\t\tthis.sink = sink;\n\t\t\tthis.position = new AtomicLong(position);\n\t\t\tthis.dataBufferFactory = dataBufferFactory;\n\t\t\tthis.bufferSize = bufferSize;\n\t\t}\n\n\t\t/**\n\t\t * Invoked when Reactive Streams consumer signals demand.\n\t\t */\n\t\tpublic void request(long n) {\n\t\t\ttryRead();\n\t\t}\n\n\t\t/**\n\t\t * Invoked when Reactive Streams consumer cancels.\n\t\t */\n\t\tpublic void cancel() {\n\t\t\tthis.state.getAndSet(State.DISPOSED);\n\n\t\t\t// According java.nio.channels.AsynchronousChannel \"if an I/O operation is outstanding\n\t\t\t// on the channel and the channel's close method is invoked, then the I/O operation\n\t\t\t// fails with the exception AsynchronousCloseException\". That should invoke the failed\n\t\t\t// callback below and the current DataBuffer should be released.\n\n\t\t\tcloseChannel(this.channel);\n\t\t}\n\n\t\tprivate void tryRead() {\n\t\t\tif (this.sink.requestedFromDownstream() > 0 && this.state.compareAndSet(State.IDLE, State.READING)) {\n\t\t\t\tread();\n\t\t\t}\n\t\t}\n\n\t\tprivate void read() {\n\t\t\tDataBuffer dataBuffer = this.dataBufferFactory.allocateBuffer(this.bufferSize);\n\t\t\tDataBuffer.ByteBufferIterator iterator = dataBuffer.writableByteBuffers();\n\t\t\tAssert.state(iterator.hasNext(), \"No ByteBuffer available\");\n\t\t\tByteBuffer byteBuffer = iterator.next();\n\t\t\tAttachment attachment = new Attachment(dataBuffer, iterator);\n\t\t\tthis.channel.read(byteBuffer, this.position.get(), attachment, this);\n\t\t}\n\n\t\t@Override\n\t\tpublic void completed(Integer read, Attachment attachment) {\n\t\t\tattachment.iterator().close();\n\t\t\tDataBuffer dataBuffer = attachment.dataBuffer();\n\n\t\t\tif (this.state.get() == State.DISPOSED) {\n\t\t\t\trelease(dataBuffer);\n\t\t\t\tcloseChannel(this.channel);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tif (read == -1) {\n\t\t\t\trelease(dataBuffer);\n\t\t\t\tcloseChannel(this.channel);\n\t\t\t\tthis.state.set(State.DISPOSED);\n\t\t\t\tthis.sink.complete();\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tthis.position.addAndGet(read);\n\t\t\tdataBuffer.writePosition(read);\n\t\t\tthis.sink.next(dataBuffer);\n\n\t\t\t// Stay in READING mode if there is demand\n\t\t\tif (this.sink.requestedFromDownstream() > 0) {\n\t\t\t\tread();\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\t// Release READING mode and then try again in case of concurrent \"request\"\n\t\t\tif (this.state.compareAndSet(State.READING, State.IDLE)) {\n\t\t\t\ttryRead();\n\t\t\t}\n\t\t}\n\n\t\t@Override\n\t\tpublic void failed(Throwable ex, Attachment attachment) {\n\t\t\tattachment.iterator().close();\n\t\t\trelease(attachment.dataBuffer());\n\n\t\t\tcloseChannel(this.channel);\n\t\t\tthis.state.set(State.DISPOSED);\n\t\t\tthis.sink.error(ex);\n\t\t}\n\n\t\tprivate enum State {\n\t\t\tIDLE, READING, DISPOSED\n\t\t}\n\n\t\tprivate record Attachment(DataBuffer dataBuffer, DataBuffer.ByteBufferIterator iterator) {}\n\t}\n\n\n\tprivate static class WritableByteChannelSubscriber extends BaseSubscriber<DataBuffer> {\n\n\t\tprivate final FluxSink<DataBuffer> sink;\n\n\t\tprivate final WritableByteChannel channel;\n\n\t\tpublic WritableByteChannelSubscriber(FluxSink<DataBuffer> sink, WritableByteChannel channel) {\n\t\t\tthis.sink = sink;\n\t\t\tthis.channel = channel;\n\t\t}\n\n\t\t@Override\n\t\tprotected void hookOnSubscribe(Subscription subscription) {\n\t\t\trequest(1);\n\t\t}\n\n\t\t@Override\n\t\tprotected void hookOnNext(DataBuffer dataBuffer) {\n\t\t\ttry {\n\t\t\t\ttry (DataBuffer.ByteBufferIterator iterator = dataBuffer.readableByteBuffers()) {\n\t\t\t\t\tByteBuffer byteBuffer = iterator.next();\n\t\t\t\t\twhile (byteBuffer.hasRemaining()) {\n\t\t\t\t\t\tthis.channel.write(byteBuffer);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tthis.sink.next(dataBuffer);\n\t\t\t\trequest(1);\n\t\t\t}\n\t\t\tcatch (IOException ex) {\n\t\t\t\tthis.sink.next(dataBuffer);\n\t\t\t\tthis.sink.error(ex);\n\t\t\t}\n\t\t}\n\n\t\t@Override\n\t\tprotected void hookOnError(Throwable throwable) {\n\t\t\tthis.sink.error(throwable);\n\t\t}\n\n\t\t@Override\n\t\tprotected void hookOnComplete() {\n\t\t\tthis.sink.complete();\n\t\t}\n\n\t\t@Override\n\t\tpublic Context currentContext() {\n\t\t\treturn Context.of(this.sink.contextView());\n\t\t}\n\t}\n\n\n\tprivate static class WriteCompletionHandler extends BaseSubscriber<DataBuffer>\n\t\t\timplements CompletionHandler<Integer, WriteCompletionHandler.Attachment> {\n\n\t\tprivate final FluxSink<DataBuffer> sink;\n\n\t\tprivate final AsynchronousFileChannel channel;\n\n\t\tprivate final AtomicBoolean writing = new AtomicBoolean();\n\n\t\tprivate final AtomicBoolean completed = new AtomicBoolean();\n\n\t\tprivate final AtomicReference<Throwable> error = new AtomicReference<>();\n\n\t\tprivate final AtomicLong position;\n\n\t\tpublic WriteCompletionHandler(\n\t\t\t\tFluxSink<DataBuffer> sink, AsynchronousFileChannel channel, long position) {\n\n\t\t\tthis.sink = sink;\n\t\t\tthis.channel = channel;\n\t\t\tthis.position = new AtomicLong(position);\n\t\t}\n\n\t\t@Override\n\t\tprotected void hookOnSubscribe(Subscription subscription) {\n\t\t\trequest(1);\n\t\t}\n\n\t\t@Override\n\t\tprotected void hookOnNext(DataBuffer dataBuffer) {\n\t\t\tDataBuffer.ByteBufferIterator iterator = dataBuffer.readableByteBuffers();\n\t\t\tif (iterator.hasNext()) {\n\t\t\t\tByteBuffer byteBuffer = iterator.next();\n\t\t\t\tlong pos = this.position.get();\n\t\t\t\tAttachment attachment = new Attachment(byteBuffer, dataBuffer, iterator);\n\t\t\t\tthis.writing.set(true);\n\t\t\t\tthis.channel.write(byteBuffer, pos, attachment, this);\n\t\t\t}\n\t\t}\n\n\t\t@Override\n\t\tprotected void hookOnError(Throwable throwable) {\n\t\t\tthis.error.set(throwable);\n\n\t\t\tif (!this.writing.get()) {\n\t\t\t\tthis.sink.error(throwable);\n\t\t\t}\n\t\t}\n\n\t\t@Override\n\t\tprotected void hookOnComplete() {\n\t\t\tthis.completed.set(true);\n\n\t\t\tif (!this.writing.get()) {\n\t\t\t\tthis.sink.complete();\n\t\t\t}\n\t\t}\n\n\t\t@Override\n\t\tpublic void completed(Integer written, Attachment attachment) {\n\t\t\tDataBuffer.ByteBufferIterator iterator = attachment.iterator();\n\t\t\titerator.close();\n\n\t\t\tlong pos = this.position.addAndGet(written);\n\t\t\tByteBuffer byteBuffer = attachment.byteBuffer();\n\n\t\t\tif (byteBuffer.hasRemaining()) {\n\t\t\t\tthis.channel.write(byteBuffer, pos, attachment, this);\n\t\t\t}\n\t\t\telse if (iterator.hasNext()) {\n\t\t\t\tByteBuffer next = iterator.next();\n\t\t\t\tthis.channel.write(next, pos, attachment, this);\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthis.sink.next(attachment.dataBuffer());\n\t\t\t\tthis.writing.set(false);\n\n\t\t\t\tThrowable throwable = this.error.get();\n\t\t\t\tif (throwable != null) {\n\t\t\t\t\tthis.sink.error(throwable);\n\t\t\t\t}\n\t\t\t\telse if (this.completed.get()) {\n\t\t\t\t\tthis.sink.complete();\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\trequest(1);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t@Override\n\t\tpublic void failed(Throwable ex, Attachment attachment) {\n\t\t\tattachment.iterator().close();\n\n\t\t\tthis.sink.next(attachment.dataBuffer());\n\t\t\tthis.writing.set(false);\n\n\t\t\tthis.sink.error(ex);\n\t\t}\n\n\t\t@Override\n\t\tpublic Context currentContext() {\n\t\t\treturn Context.of(this.sink.contextView());\n\t\t}\n\n\t\tprivate record Attachment(ByteBuffer byteBuffer, DataBuffer dataBuffer, DataBuffer.ByteBufferIterator iterator) {}\n\t}\n\n\n\tprivate static final class DataBufferMapper implements OutputStreamPublisher.ByteMapper<DataBuffer> {\n\n\t\tprivate final DataBufferFactory bufferFactory;\n\n\t\tprivate DataBufferMapper(DataBufferFactory bufferFactory) {\n\t\t\tthis.bufferFactory = bufferFactory;\n\t\t}\n\n\t\t@Override\n\t\tpublic DataBuffer map(int b) {\n\t\t\tDataBuffer buffer = this.bufferFactory.allocateBuffer(1);\n\t\t\tbuffer.write((byte) b);\n\t\t\treturn buffer;\n\t\t}\n\n\t\t@Override\n\t\tpublic DataBuffer map(byte[] b, int off, int len) {\n\t\t\tDataBuffer buffer = this.bufferFactory.allocateBuffer(len);\n\t\t\tbuffer.write(b, off, len);\n\t\t\treturn buffer;\n\t\t}\n\n\t}\n\n}\n",
    "tailType": "class_code"
  },
  {
    "head": "org.springframework.core.io.buffer.WriteCompletionHandler",
    "headType": "class",
    "relation": "haveField",
    "tail": "org.springframework.core.io.buffer.WriteCompletionHandler#sink",
    "tailType": "field"
  },
  {
    "head": "org.springframework.core.io.buffer.WriteCompletionHandler#sink",
    "headType": "field",
    "relation": "haveType",
    "tail": "FluxSink<DataBuffer>",
    "tailType": "type"
  },
  {
    "head": "org.springframework.core.io.buffer.WriteCompletionHandler",
    "headType": "class",
    "relation": "haveField",
    "tail": "org.springframework.core.io.buffer.WriteCompletionHandler#channel",
    "tailType": "field"
  },
  {
    "head": "org.springframework.core.io.buffer.WriteCompletionHandler#channel",
    "headType": "field",
    "relation": "haveType",
    "tail": "AsynchronousFileChannel",
    "tailType": "type"
  },
  {
    "head": "org.springframework.core.io.buffer.WriteCompletionHandler",
    "headType": "class",
    "relation": "haveField",
    "tail": "org.springframework.core.io.buffer.WriteCompletionHandler#writing",
    "tailType": "field"
  },
  {
    "head": "org.springframework.core.io.buffer.WriteCompletionHandler#writing",
    "headType": "field",
    "relation": "haveType",
    "tail": "AtomicBoolean",
    "tailType": "type"
  },
  {
    "head": "org.springframework.core.io.buffer.WriteCompletionHandler",
    "headType": "class",
    "relation": "haveField",
    "tail": "org.springframework.core.io.buffer.WriteCompletionHandler#completed",
    "tailType": "field"
  },
  {
    "head": "org.springframework.core.io.buffer.WriteCompletionHandler#completed",
    "headType": "field",
    "relation": "haveType",
    "tail": "AtomicBoolean",
    "tailType": "type"
  },
  {
    "head": "org.springframework.core.io.buffer.WriteCompletionHandler",
    "headType": "class",
    "relation": "haveField",
    "tail": "org.springframework.core.io.buffer.WriteCompletionHandler#error",
    "tailType": "field"
  },
  {
    "head": "org.springframework.core.io.buffer.WriteCompletionHandler#error",
    "headType": "field",
    "relation": "haveType",
    "tail": "AtomicReference<Throwable>",
    "tailType": "type"
  },
  {
    "head": "org.springframework.core.io.buffer.WriteCompletionHandler",
    "headType": "class",
    "relation": "haveField",
    "tail": "org.springframework.core.io.buffer.WriteCompletionHandler#position",
    "tailType": "field"
  },
  {
    "head": "org.springframework.core.io.buffer.WriteCompletionHandler#position",
    "headType": "field",
    "relation": "haveType",
    "tail": "AtomicLong",
    "tailType": "type"
  },
  {
    "head": "org.springframework.core.io.buffer.WriteCompletionHandler#hookOnSubscribe(Subscription)",
    "headType": "method",
    "relation": "provide",
    "tail": "@Override\r\nprotected void hookOnSubscribe(Subscription subscription) {\r\n    request(1);\r\n}",
    "tailType": "method_code"
  },
  {
    "head": "org.springframework.core.io.buffer.WriteCompletionHandler",
    "headType": "class",
    "relation": "haveMethod",
    "tail": "org.springframework.core.io.buffer.WriteCompletionHandler#hookOnSubscribe(Subscription)",
    "tailType": "method"
  },
  {
    "head": "org.springframework.core.io.buffer.WriteCompletionHandler#hookOnSubscribe(Subscription)",
    "headType": "method",
    "relation": "use",
    "tail": "@Override",
    "tailType": "annotation"
  },
  {
    "head": "org.springframework.core.io.buffer.WriteCompletionHandler#hookOnNext(DataBuffer)",
    "headType": "method",
    "relation": "provide",
    "tail": "@Override\r\nprotected void hookOnNext(DataBuffer dataBuffer) {\r\n    DataBuffer.ByteBufferIterator iterator = dataBuffer.readableByteBuffers();\r\n    if (iterator.hasNext()) {\r\n        ByteBuffer byteBuffer = iterator.next();\r\n        long pos = this.position.get();\r\n        Attachment attachment = new Attachment(byteBuffer, dataBuffer, iterator);\r\n        this.writing.set(true);\r\n        this.channel.write(byteBuffer, pos, attachment, this);\r\n    }\r\n}",
    "tailType": "method_code"
  }
]